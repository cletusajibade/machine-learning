{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(\"dataset\", \"irish\",\"train.txt\")\n",
    "test_path = os.path.join(\"dataset\", \"irish\",\"test.txt\")\n",
    "clean_data_path = os.path.join(\"dataset\", \"irish\",\"clean_train2.txt\")\n",
    "saved_model_path = os.path.join(\"dataset\", \"irish\",\"my_model_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in training data\n",
    "f = open(train_path, 'r', encoding='utf8')\n",
    "train_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    cleaned_file_out = open(clean_data_path, 'w+', encoding='utf8')\n",
    "    for i, line in enumerate(data.split(\"\\n\")):\n",
    "        new_line = line.translate(str.maketrans('', '', string.punctuation+'01234567890'))\n",
    "        new_line = new_line.lower()\n",
    "        new_line = new_line.strip()\n",
    "        new_line = \" \".join(new_line.split())\n",
    "        cleaned_file_out.write(new_line+'\\n')\n",
    "\n",
    "    cleaned_file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean the data\n",
    "train_data = clean_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cleaned data\n",
    "f = open(train_path, 'r', encoding='utf8')\n",
    "train_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word tokens from the cleaned training data\n",
    "train_data = nltk.word_tokenize(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary size to be used for Laplace Smoothing (Add-1 smoothing) later\n",
    "# the \"isalpha()\" function makes sure we filter out all non-alphabetic characters\n",
    "\n",
    "#V = len(set(word.lower() for word in train_data if word.isalpha()))\n",
    "#V = len(set(word for word in train_data if word.isalpha()))\n",
    "#V = len(set(word.lower() for word in train_data))\n",
    "V = len(set(word for word in train_data))\n",
    "#V = 10000000\n",
    "#V = len(sorted(list(set(train_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bigrams of training data\n",
    "train_bigrams = nltk.bigrams(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unigrams = {}\n",
    "\n",
    "for word in train_data:\n",
    "    count_unigrams[word] = count_unigrams.get(word, 0) + 1\n",
    "\n",
    "total_train_unigrams = sum(count_unigrams.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_bigrams = {}\n",
    "\n",
    "l = list(train_bigrams)\n",
    "\n",
    "for bg in l:\n",
    "    count_bigrams[bg] = count_bigrams.get(bg, 0) + 1\n",
    "\n",
    "total_train_bigrams = sum(count_bigrams.values())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make each prediction file different, so append current time to file name\n",
    "currentDT = str(datetime.now())\n",
    "currentDT = currentDT.replace(\" \",\"_\").replace(\":\",\"_\")\n",
    "file_name = \"my_submission_\"+currentDT+\".csv\"\n",
    "\n",
    "output_file_path = os.path.join(\"dataset\", \"irish\",\"submissions\",file_name)\n",
    "\n",
    "# Open the file for appending (a); create it if it does not aready exist (a+)\n",
    "file_out = open(output_file_path, 'a+', encoding='utf8')\n",
    "\n",
    "# First write the header in the file, as given in the sample submission\n",
    "file_out.write(\"Id,Expected\\n\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for test_sentence in open(test_path, 'r', encoding='utf8'):\n",
    "    \n",
    "    counter = counter + 1\n",
    "    \n",
    "    test_sentence = test_sentence.strip()\n",
    "    l = re.split(\"(?<={).*?(?=})\",test_sentence)\n",
    "    words_left = l[0][:-1].strip()\n",
    "    words_right = l[1][1:].strip()\n",
    "\n",
    "    match = re.search(r'.*?\\{(.*)}.*', test_sentence)\n",
    "    pairs = match.group(1).split('|')\n",
    "    pair1 = pairs[0].lower()\n",
    "    pair2 = pairs[1].lower()\n",
    "\n",
    "    words_left = nltk.word_tokenize(words_left)\n",
    "    words_right = nltk.word_tokenize(words_right)\n",
    "\n",
    "    if len(words_left) > 0 :\n",
    "        w1 = words_left[-1:][0].lower()\n",
    "    else:\n",
    "        w1 = \"\"\n",
    "        \n",
    "    if len(words_right) > 0 :\n",
    "        w2 = words_right[0:][0].lower()\n",
    "    else:\n",
    "        w2 = \"\"\n",
    "    \n",
    "    \n",
    "    # add-alpha\n",
    "    a = 1 # when alpha = 1, this is equivalent to Laplace Smoothing (Add-1 smoothing)  \n",
    "    \n",
    "    # Apply Laplace Smoothing (Add 1 and divide by V)    \n",
    "    p1 = (count_bigrams.get((w1, pair1), 0) + a)/(count_unigrams.get(w1, 0) + a*V)\n",
    "    p2 = (count_bigrams.get((pair1, w2), 0) + a)/(count_unigrams.get(pair1, 0) + a*V)\n",
    "    \n",
    "    alpha =p1*p2\n",
    "       \n",
    "    p1 = (count_bigrams.get((w1, pair2), 0) + a)/(count_unigrams.get(w1, 0) + a*V)\n",
    "    p2 = (count_bigrams.get((pair2, w2), 0) + a)/(count_unigrams.get(pair2, 0) + a*V)\n",
    "        \n",
    "    beta =p1*p2\n",
    "        \n",
    "    prob = alpha/(alpha+beta)\n",
    "    \n",
    "    # Build each line and write it to the output file\n",
    "    data = str(counter)+\",\"+str(prob)+\"\\n\"\n",
    "    file_out.write(data)\n",
    "\n",
    "file_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
